Main Functionalities
----------------------------------


Basic Walk-through
******************

The following is a basic example of using XGBoost to train an extreme gradient boosting model using `Mushroom Data Set <https://archive.ics.uci.edu/ml/datasets/Mushroom>`_ from `UCI Machine Learning Repository <https://archive.ics.uci.edu/ml/datasets>`_. You can find a binary buffer of the data set generated by XGBoost `here <https://github.com/dmlc/xgboost/tree/master/demo/data>`_. XGBoost uses an internal data structure called `DMatrix`, which is optimized for both memory efficiency and training speed. Various types of data sources can be passed in to construct `DMatrix` object, such as Numpy array, Scipy sparse matrix, Pandas data frame, or a string that represents the path to a `LIBSVM format <http://www.csie.ntu.edu.tw/~cjlin/libsvm/>`_ or a binary file like the following. 

.. code-block:: python

	import xgboost as xgb
	import numpy as np

	dtrain = xgb.DMatrix('agaricus.txt.train')
	dtest = xgb.DMatrix('agaricus.txt.test')

	param = {'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic' }
	watchlist = [(dtest,'eval'), (dtrain,'train')]
	num_round = 2
	bst = xgb.train(param, dtrain, num_round, watchlist)
	preds = bst.predict(dtest)
	labels = dtest.get_label()
	err = sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i]) / float(len(preds))


In the above example, we first load the binary file into a `DMatrix` object that can later be fed into XGBoost. We specify all the parameters used to train the booster in a dictionary as well as the objective of the model, which in this case is logistic regression for binary classification problem. A more thorough guide to help you understand and tune your parameters is illustrated in the next section. We then specify a list of tuples where each tuple represents a pair of (DMatrix, string) so you can watch the results during training. For example, if you specify `'eval_metric': 'logloss'` as an item in the above `param` dictionary and define the watchlist as in above example, you will see the provided metrics being evaluated during both training and evaluation period like the following: 

.. code-block:: python

	{'train': {'logloss': ['0.48253', '0.35953']},
	 'eval': {'logloss': ['0.480385', '0.357756']}}

Once a model is trained, we can easily save the model to disk and load it back again later on. The foolowing example saves the model as well as the `DMatrix` binary buffer from previous example so we can see whether the model loaded from previously saved model gives us the same predictions on the same test data set.

.. code-block:: python

	bst.save_model('xgb.model')
	bst2 = xgb.Booster(model_file='xgb.model')
	dtest2 = xgb.DMatrix('dtest.buffer')
	preds2 = bst2.predict(dtest2)
	assert np.sum(np.abs(preds2-preds)) == 0



Custom Objective and Evaluation Function
****************************************

In XGBoost, we can provide custom objective function and evaluation function for training the model. They are often used for many applications so the model reflects more true characteristics of the real problems. 

.. code-block:: python

	dtrain = xgb.DMatrix('../data/agaricus.txt.train')
	dtest = xgb.DMatrix('../data/agaricus.txt.test')

	param = {'max_depth': 2, 'eta': 1, 'silent': 1}
	watchlist = [(dtest, 'eval'), (dtrain, 'train')]
	num_round = 2

	def logregobj(preds, dtrain):
	    labels = dtrain.get_label()
	    preds = 1.0 / (1.0 + np.exp(-preds))
	    grad = preds - labels
	    hess = preds * (1.0-preds)
	    return grad, hess

	def evalerror(preds, dtrain):
	    labels = dtrain.get_label()
	    return 'error', float(sum(labels != (preds > 0.0))) / len(labels)

	bst = xgb.train(param, dtrain, num_round, watchlist, logregobj, evalerror)

We first load the training and test set as usual and define necessary information such as parameters, watch list, and the number of rounds for training. Note that for customized objective function, we leave objective in `params` as default. Then we define customized object function that accepts the prediction probabilities `preds` and the training set `dtrain`, and then returns the gradient and second order gradient so in this case our objective is the log-likelihood loss. 

Similarly, our customized evaluation function accepts the same input but returns a tuple with metric name as a string and the result of the metric. We then pass the customized functions into `xgb.train` to train our model using them instead of built-in objectives and metrics. The customized evaluation function can also return a list of tuples, for example:

.. code-block:: python

	def evalerror_03(self, preds, dtrain):
        from sklearn.metrics import mean_squared_error
        labels = dtrain.get_label()
        return [('rmse', mean_squared_error(labels, preds)),
                ('error', float(sum(labels != (preds > 0.0))) / len(labels))]


Boost from Existing Prediction
****************************************

Users can set the base margin of booster to start training from. This can be used to specify a base margin for the prediction value of each data point in existing model. For example, let's say we have a booster object trained like the following:

.. code-block:: python

	dtrain = xgb.DMatrix('data/agaricus.txt.train')
	dtest = xgb.DMatrix('data/agaricus.txt.test')
	watchlist  = [(dtest, 'eval'), (dtrain, 'train')]
	params = {'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}
	bst = xgb.train(params, dtrain, 1, watchlist)

We can then output the raw untransformed margin value during prediction, using `output_margin=True` in `predict()`. We then use `set_base_margin()` to set the returned untransformed margin values to be the base margin of the `DMatrix` objects that we use for training and testing, and then we resume boosting from the existing predictions.

.. code-block:: python

	ptrain = bst.predict(dtrain, output_margin=True)
	ptest  = bst.predict(dtest, output_margin=True)
	dtrain.set_base_margin(ptrain)
	dtest.set_base_margin(ptest)
	bst = xgb.train(params, dtrain, 1, watchlist)


Predict Using First N Trees
****************************************

We can also predict using only limited number of trees we specify. This is available via the `ntree_limit` argument in `predict()`. For example, below we use only the first 3 trees from the booster:

.. code-block:: python

	dtrain = xgb.DMatrix('data/agaricus.txt.train')
	dtest = xgb.DMatrix('data/agaricus.txt.test')
	param = {'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}
	watchlist  = [(dtest, 'eval'), (dtrain, 'train')]
	num_round = 3
	bst = xgb.train(param, dtrain, num_round, watchlist)
	ypred = bst.predict(dtest, ntree_limit=3)


Predict Leaf Indices
****************************************

There's also `pred_leaf` argument you can use in `predict()` to obtain the predicted leaf indices of each sample. If `pred_leaf` option is turned on, a matrix will be returned with the shape of `(nsamples, ntrees)` where each record indicating the predicted indices of each sample in each tree. It's worth noting that the leaf indices of a tree is unique per tree so it's possible, for example, to obtain leaf 1 in both tree 1 and tree 0. This feature is extremely useful when users want to understand the details of the splitting. Users can even discover interesting insight when tree starts to split in an unusual direction. There are also users who use these predicted leaf indices as additional features fed into the model. The following is an example on how to use this feature:

.. code-block:: python

	dtrain = xgb.DMatrix('data/agaricus.txt.train')
	dtest = xgb.DMatrix('data/agaricus.txt.test')
	param = {'max_depth':2, 'eta':1, 'silent':1, 'objective': 'binary:logistic'}
	watchlist  = [(dtest, 'eval'), (dtrain, 'train')]
	num_round = 3
	bst = xgb.train(param, dtrain, num_round, watchlist)
	leafindex = bst.predict(dtest, ntree_limit=2, pred_leaf=True)


Cross Validation
****************************************

XGBoost provides cross validation function that users can use to partition the data into complementary subsets to be used for training and testing separately. This is a very popular approach used in machine learning that could greatly reduce the variability of a model. In other words, it's used very often to access how generalizable a model is on independent data sets. For example, we use `cv()` to do this like follows:

.. code-block:: python

	dtrain = xgb.DMatrix('data/agaricus.txt.train')
	param = {'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}
	num_round = 2
	xgb.cv(param, dtrain, num_round, nfold=5,
	       metrics={'error'}, seed=0)

Callbacks can be passed into `cv()` to monitor and control the process. For example, here we use `print_evaluation()` from `callback` module to print out standard deviation and use `early_stop()` to provide the necessary logics to be used to stop the training process when certain conditions are met.

.. code-block:: python

	res = xgb.cv(param, dtrain, num_boost_round=10, nfold=5,
	             metrics={'error'}, seed=0,
	             callbacks=[xgb.callback.print_evaluation(show_stdv=True),
	                        xgb.callback.early_stop(3)])

Users can also define customized pre-processing function used to pre-process training and testing set, as well as parameters that might be changed dynamically inside this function. For example, we can define the pre-processing function as follows to rescale the weights:

.. code-block:: python

	def fpreproc(dtrain, dtest, param):
	    label = dtrain.get_label()
	    ratio = float(np.sum(label == 0)) / np.sum(label == 1)
	    param['scale_pos_weight'] = ratio
	    return (dtrain, dtest, param)

For each fold, the training and testing set, as well as hyper-parameters will be passed into the defined function `fpreproc()` and the returned value of those will be used to generate results of each particular fold. We then pass `fpreproc` into `cv()` to use previously defined pre-processing function:

.. code-block:: python

	xgb.cv(param, dtrain, num_round, nfold=5, metrics={'auc'},
		   seed=0, fpreproc=fpreproc)

We can also conduct cross-validation using customized objective and evaluation function. Very similar to what we've discussed in previous sections:

.. code-block:: python

	def logregobj(preds, dtrain):
	    labels = dtrain.get_label()
	    preds = 1.0 / (1.0 + np.exp(-preds))
	    grad = preds - labels
	    hess = preds * (1.0-preds)
	    return grad, hess

	def evalerror(preds, dtrain):
	    labels = dtrain.get_label()
	    return 'error', float(sum(labels != (preds > 0.0))) / len(labels)

	param = {'max_depth':2, 'eta':1, 'silent':1}
	xgb.cv(param, dtrain, num_round, nfold=5, seed=0,
	       obj=logregobj, feval=evalerror)


Scikit-learn Wrapper
*****************************************

XGBoost provides a wrapper class to allow models to be treated like classifiers or regressors in the Scikit-learn framework. This allows users to use XGBoost along with features of Scikit-learn easily. For example, the following is an example of using `XGBClassifier` to conduct binary classification on digits dataset imported from `sklearn.datasets`. Here we use `KFold` from Scikit-learn to split the data into 2 folds of training and testing set, and then use `XGBClassifier.fit()` and `XGBClassifier.predict()` to train and test the model on every fold respectively. 

.. code-block:: python
	
    from sklearn.datasets import load_digits
    from sklearn.cross_validation import KFold

    digits = load_digits(2)
    y = digits['target']
    X = digits['data']
    kf = KFold(y.shape[0], n_folds=2, shuffle=True, random_state=rng)
    for train_index, test_index in kf:
        xgb_model = xgb.XGBClassifier().fit(X[train_index], y[train_index])
        preds = xgb_model.predict(X[test_index])
        labels = y[test_index]
        err = sum(1 for i in range(len(preds))
                  if int(preds[i] > 0.5) != labels[i]) / float(len(preds))

Similarly, we can train a regression model on boston housing data imported from `sklearn.datasets` using `XGBRegressor`. Similar to above example, we use familiar syntax from Scikit-learn to fit the model and generate predictions. Additionally, the parameters in `xgboost.Booster.predict()` such as `output_margin` and `ntree_limit` introduced in previous sections are also available, as shown in the below example:

.. code-block:: python
	
	from sklearn.metrics import mean_squared_error
	from sklearn.datasets import load_boston
	from sklearn.cross_validation import KFold

	boston = load_boston()
	y = boston['target']
	X = boston['data']
	kf = KFold(y.shape[0], n_folds=2, shuffle=True, random_state=rng)
	for train_index, test_index in kf:
	    xgb_model = xgb.XGBRegressor().fit(X[train_index], y[train_index])
	    preds1 = xgb_model.predict(X[test_index])
	    preds2 = xgb_model.predict(X[test_index], output_margin=True, ntree_limit=3)
	    preds3 = xgb_model.predict(X[test_index], output_margin=False, ntree_limit=1)

Furthermore, we can fully take advantage of other modules and helper functions, such as `grid_search.GridSearchCV`, to streamline model building and developing process. We pass a list of hyper-parameters to `grid_search.GridSearchCV` and then call `fit()` to conduct the grid search on optimal hyper-parameters. 

.. code-block:: python

	from sklearn.grid_search import GridSearchCV
	from sklearn.datasets import load_boston

	boston = load_boston()
	y = boston['target']
	X = boston['data']
	xgb_model = xgb.XGBRegressor()
	clf = GridSearchCV(xgb_model, {'max_depth': [2, 4, 6],
	                               'n_estimators': [50, 100, 200]}, verbose=1)
	clf.fit(X, y)
