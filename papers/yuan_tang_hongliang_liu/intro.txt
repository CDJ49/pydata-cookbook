Introduction
==================================

Machine learning approaches are becoming more and more important nowadays:
recommendation systems learn to generate recommendations of potential interest
with the right context according to user feedback and interactions; anomalous
event detection systems helps monitor assets to avoids downtime due to extreme
conditions; faud detection systems protect financial institutions from security
attacks and malicious fraud behaviors. Among the popular machine learning methods
used in industrial applications, gradient tree boosting is one method that stands
out due to its state-of-the-art results on many classification benchmarks.

XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting
library designed to be highly efficient, flexible and portable. XGBoost provides
a parallel tree boosting (also known as GBDT, GBM) that solve many data science
problems in a fast and accurate way. It is widely used by data scientists to
achieve state-of-the-art results on many machine learning competitions. XGBoost
can run on various environments, including single machine as well as distributed
envionrments such as Hadoop, MPI, Apache Spark, Apache Flink, etc. It provides
language bindings such as Python, R, Java, Scala, C++, etc. The library includes
efficient linear model solver and tree learning algorithms. It supports various
objective functions, including regression, classification and ranking and it's
made to be extensible so that users are also allowed to define their custom
objectives easily.

XGBoost system gives state-of-the-art results on a wide range of problems.
Some winning solutions for Kaggle competitions that used XGBoost include:
predicting if individual users will click a given context ad; predicting the
price a supplier will quote for a given tube assembly; predicting massive online
course dropout rate; classifing high energy physics events. It's also being
deployed and incorporated into real-world production pipelines such as Tencent's
click-through rate prediction and Alibaba's ODPSCloud service.

The success of XGBoost lies in its scalability in many scenarios.
For example, it runs at least ten times faster than existing popular
solutions on a single machine and scales to billions of examples in distributed
or memory-limited settings such as edge devices. XGBoost has been through several
important system level and algorithmic optimizations to achieve such scalability.
The optimizations include: a novel tree learning algorithm is for handling sparse
data; a theoretically justified weighted quantile sketch procedure enables
handling instance weights in approximate tree learning. Additionally, XGBoost
employs a lot of parallel and distributed computing processes that make learning
faster to enable quicker model exploration. It exploits out-of-core computation
and enables data scientists to process hundred millions of data records on a
desktop.
